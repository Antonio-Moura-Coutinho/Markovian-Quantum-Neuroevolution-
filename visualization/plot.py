import numpy as np 
from matplotlib import pyplot as plt 
import scipy.io

# copied directly from log_evolve
merits = np.array([[0.39, 0.39, 0.39, 0.55, 0.76, 0.84, 0.555, 0.72, 0.65, 0.39], [0.855, 0.76, 0.775, 0.72, 0.825, 0.68, 0.775, 0.77, 0.825, 0.655], [0.83, 0.875, 0.965, 0.55, 0.675, 0.695, 0.735, 0.69, 0.82, 0.835], [0.915, 0.965, 0.91, 0.955, 0.88, 0.97, 0.955, 0.955, 0.955, 0.84], [0.915, 0.815, 0.9, 0.955, 0.965, 0.935, 0.905, 0.975, 0.83, 0.88], [0.96, 0.945, 0.945, 0.955, 0.96, 0.755, 0.95, 0.935, 0.935, 0.95], [0.94, 0.945, 0.955, 0.975, 0.895, 0.96, 0.895, 0.915, 0.96, 0.97], [0.975, 0.94, 0.975, 0.925, 0.95, 0.955, 0.97, 0.965, 0.965, 0.945], [0.955, 0.95, 0.975, 0.955, 0.955, 0.965, 0.97, 0.965, 0.94, 0.98], [0.94, 0.975, 0.955, 0.93, 0.965, 0.975, 0.97, 0.95, 0.95, 0.955]])
train_loss = np.array([0.7133775867774249, 0.7074467100725863, 0.7016634592886158, 0.6960520655945568, 0.6907129350286573, 0.6856923845276175, 0.6811405752763049, 0.6770956911600652, 0.6731703067232291, 0.6694681100916596, 0.6661413697863457, 0.6631857931703419, 0.6604911058271142, 0.6580761385456299, 0.6559234875204282, 0.6539470742643723, 0.6521268137378431, 0.6504393461064713, 0.6488576226565318, 0.647419003126623, 0.6460235629094142, 0.6448567326530776, 0.6437211722538001, 0.6425380727060017, 0.6411803685534164, 0.6397672601585178, 0.6384024982986412, 0.6371887946883659, 0.6358635230635551, 0.6344702861371401, 0.633034557851043, 0.6315861507728656, 0.6300996611655748, 0.6286064807798807, 0.6270732227778569, 0.6255898637983339, 0.6240684657595408, 0.6224926828828047, 0.6209961054999552, 0.6194765519636721, 0.6179683623614898, 0.6165285322520472, 0.6151871059911752, 0.6139597455517115, 0.6128647940521132, 0.6118585208684663, 0.6108298516986367, 0.6099202100376544, 0.6091400909602348, 0.608050787727639, 0.6068694637495806, 0.6054553044394131, 0.6039054632956427, 0.6020695378612622, 0.6003838204143597, 0.5987715636804519, 0.5972858473974539, 0.595898927114209, 0.5946393068686944, 0.5934670859021426, 0.5924058540559968, 0.5914026923235091, 0.5901033276703256, 0.5888399627833415, 0.5875156109845129, 0.5860631238627184, 0.5846542416446262, 0.5833490014622055, 0.5821517838110055, 0.5811113044844133, 0.5801505196763598, 0.5792375064146975, 0.5781214192945945, 0.5769525868675034, 0.5756985854019435, 0.5744660447859333, 0.5732525162573587, 0.5719453090283921, 0.5706321191397355, 0.5693817483087007, 0.568121864696923, 0.5669347838645347, 0.5657777190732947, 0.5646560104776481, 0.56346780279095, 0.5622496748183874, 0.5610674908163907, 0.5598749984530714, 0.5586810880561569, 0.5575482781408096, 0.5564556506313884, 0.5553791705354977, 0.5543703456105157, 0.5534116531603417, 0.5523709421085113, 0.5514112662477767, 0.5502770538191522, 0.5489356091548622, 0.547616170132108, 0.54637918334572, 0.5451436925470121, 0.5439416077383471, 0.5427865865084056, 0.5416415117864609, 0.5405230490129906, 0.5393972942050266, 0.5382635661314707, 0.5370775932767217, 0.5358603370389132, 0.5346675261901261, 0.5334936401830452, 0.5323387688889454, 0.53120929748184, 0.5300773824757133, 0.5289750329728424, 0.5278726906747582, 0.526811138469979, 0.5258008888851107, 0.5248317906356028, 0.5238779336822053, 0.5229786910865911, 0.5220982128426068, 0.5211590767675868, 0.5200926310258454, 0.5190497184064534, 0.5180597136993099, 0.5170877432286809, 0.516139292182576, 0.5152416148807267, 0.5144092525213569, 0.5136003533305964, 0.5127763186143282, 0.5119037961531473, 0.5110348225583448, 0.510119993683988, 0.5092595541708157, 0.5084419863361582, 0.5076591838979848, 0.5069177624554598, 0.5062521270299887, 0.5055442828679808, 0.5048340929121837, 0.5041133620098918, 0.5033849998442196, 0.5027074242774662, 0.5020141337422895, 0.501337015022406, 0.5007015801836239, 0.5000460335967254, 0.4994281241573325, 0.49885335161022204])
train_acc = np.array([0.419, 0.419, 0.417, 0.407, 0.389, 0.385, 0.378, 0.39, 0.413, 0.432, 0.458, 0.504, 0.547, 0.573, 0.59, 0.607, 0.626, 0.628, 0.629, 0.623, 0.62, 0.617, 0.612, 0.603, 0.601, 0.604, 0.6, 0.589, 0.585, 0.585, 0.586, 0.591, 0.6, 0.604, 0.614, 0.62, 0.626, 0.638, 0.646, 0.661, 0.673, 0.707, 0.728, 0.76, 0.771, 0.797, 0.805, 0.811, 0.825, 0.833, 0.845, 0.848, 0.851, 0.848, 0.838, 0.833, 0.828, 0.806, 0.788, 0.773, 0.754, 0.742, 0.751, 0.761, 0.769, 0.788, 0.816, 0.831, 0.864, 0.886, 0.906, 0.914, 0.914, 0.917, 0.918, 0.92, 0.919, 0.917, 0.914, 0.912, 0.906, 0.899, 0.889, 0.882, 0.883, 0.894, 0.895, 0.899, 0.913, 0.919, 0.928, 0.934, 0.939, 0.943, 0.947, 0.948, 0.947, 0.95, 0.951, 0.952, 0.947, 0.946, 0.944, 0.947, 0.944, 0.944, 0.944, 0.947, 0.95, 0.952, 0.955, 0.959, 0.962, 0.964, 0.964, 0.964, 0.964, 0.964, 0.965, 0.966, 0.965, 0.963, 0.964, 0.966, 0.966, 0.965, 0.966, 0.966, 0.967, 0.97, 0.972, 0.971, 0.971, 0.971, 0.97, 0.971, 0.971, 0.969, 0.968, 0.967, 0.967, 0.968, 0.969, 0.97, 0.97, 0.971, 0.971, 0.971, 0.97, 0.97, 0.971])
test_loss = np.array([0.6551124216703854, 0.6506758837429554, 0.6465782041079116, 0.6428118553525383, 0.6394359266787558, 0.6365370866848086, 0.6341167012435645, 0.6322014692718975, 0.6305777019696298, 0.6293349132965769, 0.6284635224851395, 0.6279004623382667, 0.6276499113942984, 0.6276454663990969, 0.6277519207215616, 0.6280462619359227, 0.6285744784616742, 0.6291373584536489, 0.62973406922966, 0.6304102332101723, 0.6309864117720605, 0.6323266620281968, 0.633461365004912, 0.6341718616583207, 0.6339219607620534, 0.6333861586194997, 0.6330591942677005, 0.6332608840096108, 0.6328408948766395, 0.6321006819973684, 0.6311302452839267, 0.6301058972893131, 0.6289162751453934, 0.6277093645454743, 0.6262711875912603, 0.6250313171244751, 0.6236274883342963, 0.6217962469642871, 0.6204279217893263, 0.6188217342471161, 0.6168745951288741, 0.6145424242153057, 0.6123493736181519, 0.6100470856244968, 0.6077431259104134, 0.6055202286292718, 0.6036701909538562, 0.6017280892467116, 0.5998095188925155, 0.5984888067045734, 0.5972955692396748, 0.5962979811083833, 0.5954876081175875, 0.595270052874307, 0.5951378844998994, 0.5951936270970194, 0.5953021907232469, 0.5959477487490518, 0.5966272770039169, 0.5971733220702746, 0.5978007990418122, 0.5983972134136836, 0.5970138326549633, 0.5960360823454733, 0.5947524982443518, 0.5924656990469199, 0.5900556845020966, 0.588132627651133, 0.5853787981460606, 0.5831213325064784, 0.5812430063430017, 0.5796264215207217, 0.5786770441071674, 0.5778512434842838, 0.5771709834668162, 0.5764491098327367, 0.5756643390273725, 0.5752321798168952, 0.5750081890257236, 0.5746193641582097, 0.57462794305142, 0.5745100714324238, 0.5745981384776576, 0.5743951411936072, 0.5732965331954101, 0.5716027250759115, 0.5703196097378985, 0.5690042268744332, 0.5670055701192812, 0.564554300877132, 0.5626286726193483, 0.5608363570587686, 0.5589085693702949, 0.5570212624354263, 0.5554572932005829, 0.5537204696558472, 0.5523400600873026, 0.5514717813866945, 0.5507343369533596, 0.5498789701702982, 0.5491891008108307, 0.5485688447543715, 0.5477138168557517, 0.5464338516066658, 0.5456331204168715, 0.5447620696967321, 0.5436304599707981, 0.5422209957259352, 0.5403206341342447, 0.5386114178812216, 0.5369149713128227, 0.5353095490420544, 0.5337265305743706, 0.5323990729334461, 0.5312252531084237, 0.5304598647404702, 0.5298991508730184, 0.5294119578099831, 0.5289284039420827, 0.5282571691741448, 0.5276720254886349, 0.5269880288668513, 0.5259269716780854, 0.524198448047968, 0.5223913792107653, 0.5206902276573733, 0.5188585358336125, 0.516891356762255, 0.5149696684098692, 0.5130107508316383, 0.5113640397855564, 0.5099335830911047, 0.5086711473244363, 0.507541591089186, 0.5068380878477301, 0.5060149761146281, 0.5052641934521326, 0.5045368510809524, 0.5038888951079056, 0.5035425144401452, 0.502744470447686, 0.5018845727186365, 0.500893960277812, 0.4997948045214453, 0.4988653129842273, 0.4977684550961639, 0.49670180464264746, 0.49584229408839564, 0.49477012940723936, 0.49391074953423775, 0.49333374991934675])
test_acc = np.array([0.505, 0.51, 0.505, 0.495, 0.48, 0.49, 0.49, 0.52, 0.525, 0.56, 0.635, 0.68, 0.715, 0.71, 0.73, 0.73, 0.74, 0.735, 0.735, 0.735, 0.735, 0.71, 0.695, 0.67, 0.67, 0.65, 0.645, 0.625, 0.625, 0.62, 0.62, 0.625, 0.63, 0.63, 0.64, 0.645, 0.65, 0.675, 0.68, 0.71, 0.735, 0.76, 0.8, 0.825, 0.84, 0.85, 0.855, 0.875, 0.88, 0.88, 0.88, 0.88, 0.885, 0.89, 0.89, 0.895, 0.875, 0.835, 0.81, 0.8, 0.775, 0.75, 0.765, 0.765, 0.77, 0.8, 0.82, 0.835, 0.86, 0.895, 0.91, 0.93, 0.93, 0.93, 0.93, 0.925, 0.93, 0.92, 0.92, 0.92, 0.895, 0.87, 0.86, 0.845, 0.85, 0.865, 0.87, 0.88, 0.895, 0.92, 0.93, 0.93, 0.94, 0.955, 0.955, 0.955, 0.955, 0.955, 0.955, 0.955, 0.955, 0.95, 0.95, 0.955, 0.95, 0.955, 0.95, 0.955, 0.965, 0.97, 0.965, 0.965, 0.97, 0.975, 0.975, 0.975, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.975, 0.975, 0.97, 0.96, 0.955, 0.955, 0.955, 0.955, 0.955, 0.955, 0.955, 0.955, 0.955, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96, 0.96])

num_gen = merits.shape[0]
num_chains = merits.shape[1]
niters = train_loss.shape[0]
gen = np.full((num_gen, num_chains), np.arange(num_gen).reshape(num_gen,1))
epochs = np.arange(niters)

plt.plot(np.arange(num_gen), np.max(merits, axis=1), color='red', linestyle='-', linewidth=1, label="The highest fitness")
plt.scatter(gen, merits, s=2, label="All populations")

plt.xlabel('Generations')
plt.ylabel('Fitness')
plt.legend()
plt.savefig("fitness.pdf")

plt.close()


space = 4
pointsize = 30

fig, ax1 = plt.subplots()
ax1.plot(epochs, train_acc, color='blue', zorder=1, alpha=0.6)
ax1.scatter(epochs[::space], train_acc[::space], marker='o', label="Training accuracy", facecolors='white', edgecolors='blue', s=pointsize, zorder=2, alpha=0.6)
ax1.plot(epochs, test_acc, color='orange', zorder=1, alpha=0.6)
ax1.scatter(epochs[::space], test_acc[::space], marker='^', label="Validation accuracy", facecolors='white', edgecolors="orange", s=pointsize, zorder=2, alpha=0.6)
ax1.set_xlabel('Epochs') 
ax1.set_ylabel('Accuracy', color="black") 
ax1.tick_params(axis='y', labelcolor="black")  
ax1.set_ylim(0.35, 1.0)  
ax1.set_xlim(-5, niters) 

ax1.legend(loc="lower right", bbox_to_anchor=(1, 0.6))

ax2 = ax1.twinx()
ax2.plot(epochs, train_loss, color='green', zorder=1, alpha=0.6)
ax2.scatter(epochs[::space], train_loss[::space], marker='v', label="Training loss", facecolors='white', edgecolors="green", s=pointsize, zorder=2, alpha=0.6)
ax2.plot(epochs, test_loss, color='red', zorder=1, alpha=0.6)
ax2.scatter(epochs[::space], test_loss[::space], marker='s', label="Validation loss", facecolors='white', edgecolors="red", s=pointsize, zorder=2, alpha=0.6)
ax2.set_ylabel('Loss', color="black")  
ax2.tick_params(axis='y', labelcolor="black")  
ax2.set_ylim(0.45, 0.75) 
ax2.set_xlim(-5, niters)  

ax2.legend(loc="lower right", bbox_to_anchor=(1, 0.4))

plt.savefig("trainbest.pdf")
